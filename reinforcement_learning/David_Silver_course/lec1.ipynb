{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* demos\n",
    "    * helicopter\n",
    "    * atario games\n",
    "* $R_t$ is a scalar feedback signal\n",
    "* agent tries to max cumulative reward\n",
    "* agent and environemnt\n",
    "    * $O_t$, $A_t$, $R_t$\n",
    "* history $H_t = A_1, O_1, R_1, ..., A_t, O_t, R_t$\n",
    "* formally, state is a function of history $S_t = f(H_t)$\n",
    "* markov state $P(S_{t+1} | S_t) = P(S_{t+1}|S_1,...,S_t)  $\n",
    "    * really important\n",
    "* if the environemnt is only partially observable, it's called Partially Observable MDP (POMDP)\n",
    "* agent must construct its own state rep\n",
    "    * complete history -> \n",
    "    * beliefs of environment -> $S_t^a = (P[S_t^e=s^1], ..., P[S_t^e=s^n])$\n",
    "    * recurrent NN -> $S_t^a = \\sigma(S_{t-1)}^a W_s + O_t W_o)$\n",
    "* RL agent\n",
    "    * policy - agent's behavior fn\n",
    "    * value fn - how good is state / action\n",
    "    * model - agent's rep of the env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* policy - agent's behavior\n",
    "    * map from state to action e.g. $a = \\pi(s)$\n",
    "* stochastic policy $\\pi (a|s) = P(A=a|S=s)$\n",
    "* value fn - prediction of future reward\n",
    "* used to evaluate goodness or badness of state\n",
    "* $v_\\pi (s) = E_\\pi[R_t + \\gamma R_{t+1} + ... | S_t = s]$\n",
    "* model pedicts what the env will do next\n",
    "* transitions: $P$ predicts the next state i.e. dynamics\n",
    "* rewards: predicts the next reward\n",
    "* e.g. $P_{ss'}^a = P(S' = s' | S=s, A=a)$\n",
    "* $R_s^a = E[R | S=s, A=a]$\n",
    "* policy $\\pi(s)$ for a maze will be a map of directions of which way the agent will go on a square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* categorizing RL agents\n",
    "    * value based\n",
    "    * policy based\n",
    "    * actor critic\n",
    "        * policy + value \n",
    "* categorizing RL agents\n",
    "    * model free\n",
    "        * policy and / or value function\n",
    "        * no model\n",
    "    * model based\n",
    "        * model\n",
    "* 2 problems in sequential decision making\n",
    "    * RL\n",
    "        * env is initially unknown\n",
    "        * agent interacts\n",
    "        * improves policy\n",
    "    * planning\n",
    "        * model of env is known\n",
    "        * agent performs internal computation with its model\n",
    "        * improves policy\n",
    "* exploration vs exploitation\n",
    "    * discover a good policy without losing too much reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
