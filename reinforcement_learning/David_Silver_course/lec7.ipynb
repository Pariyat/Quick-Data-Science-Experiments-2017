{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[src](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy-based reinforcment learning\n",
    "* approximated action-value function via a fn $Q_\\theta(s,a) = Q^\\pi (s,a)$\n",
    "* policy is generated from the action-value function\n",
    "    * $\\epsilon$-greedy\n",
    "* in policy gradient, we directly parametrize policy\n",
    "    * $\\pi_\\theta(s,a) = P[a|s, \\theta]$\n",
    "* doing model-free reinforcement learning\n",
    "\n",
    "\n",
    "### Value-based vs Policy-based RL\n",
    "* value based\n",
    "    * learnt value fn\n",
    "    * implicit policy (e.g. $\\epsilon$-greedy)\n",
    "* policy based\n",
    "    * no value fn\n",
    "    * learnt policy\n",
    "* actor-critic\n",
    "    * somehwere in the middle\n",
    "    * learn both value and policy fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pros and cons of policy-based RL\n",
    "* adv:\n",
    "    * better convergence policy\n",
    "    * effective in high dimension or continuous action space\n",
    "    * can learn stochastic policy\n",
    "* disadv:\n",
    "    * typically converge to a local minimum\n",
    "    * evaluating a policy is highly inefficient and high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example - gridworld\n",
    "* $\\phi(s,a) = 1(s = \\text{wall top n}, a = \\text{move E})$\n",
    "* $Q_\\theta (s,a) = f(\\phi(s,a), \\theta)$ - value based\n",
    "* $\\pi_\\theta (s,a) = g(\\phi(s,a), \\theta)$ - policy based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy objective function\n",
    "* goal: given policy $\\pi_\\theta (s,a)$, find best $\\theta$\n",
    "* but how do we measure the quality of a policy?\n",
    "* in episodic env, we can use the start value \n",
    "    * $J_1(\\theta) = V^{\\pi_\\theta} (s_1) = E_{\\pi_\\theta}[v_1]$\n",
    "* in continuous env, use avg value\n",
    "    * $J_{av} (\\theta) = \\sum_s d^{\\pi_\\theta} (s) V^{\\pi_\\theta} (s)$\n",
    "    * $d$ is the stationary distribution of markov chain of $\\pi_\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy optimization\n",
    "* policy based reinforcement learning is an optimization problem\n",
    "* find $\\theta$ that maximizes $J(\\theta)$\n",
    "\n",
    "### computing gradients by finite differences\n",
    "* to evaluate policy gradient of $\\pi_\\theta(s,a)$\n",
    "* for each dimension $k \\in [1,n]$\n",
    "    * estimate kth partial derivative of objective fn wrt $\\theta$\n",
    "    * perturb $\\theta$ by small amount $\\frac{dJ}{d\\theta} = \\frac{J(\\theta+\\epsilon) - J(\\theta)}{\\epsilon}$\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score fns\n",
    "* compute policy gradient analytically\n",
    "* assume policy $\\pi_\\theta$ is differentiable whenever it is non-zero and we know the gradient $\\nabla_\\theta \\pi_\\theta$\n",
    "* likelihood ratio explores these identities $\\nabla_\\theta \\pi (s,a) = \\pi_\\theta (s,a) \\frac{\\nabla_\\theta \\pi_\\theta (s,a))}{\\pi_\\theta (s,a)} = \\pi(s,a) \\nabla log \\pi_\\theta (s,a)$\n",
    "* the score fn is $\\nabla_\\theta log \\pi_\\theta (s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax policy\n",
    "* weight actions via linear combination of features $\\phi(s,a)^T \\theta$\n",
    "* $\\pi_\\theta (s,a) \\sim e^{\\phi(s,a)^T \\theta}$\n",
    "* score fn is $\\nabla_\\theta log \\pi_\\theta (s,a) = \\phi(s,a) - E_{\\pi_\\theta} [\\phi(s,*)] $\n",
    "    * ^ just logging it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gaussian policy\n",
    "* use in the continuous action space!\n",
    "* mean is a linear combination of state features $\\mu(s) = \\phi(s)^T \\theta$\n",
    "* variance may be fixed or parameterized $\\sigma^2$\n",
    "* score fn is $\\nabla_\\theta log \\pi_\\theta(s,a) = \\frac{(a - \\mu(s))\\phi(s))}{\\sigma^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one step MDP example\n",
    "* consider a 1 step MDP, you make 1 step, get a reward and terminate\n",
    "* $J(\\theta) = E_{\\pi_\\theta} [r]$\n",
    "* $= \\sum_{s \\in S} d(s) \\sum_{a \\in A} \\pi_\\theta (s,a) R_{s,a}$\n",
    "* $\\nabla_\\theta J(\\theta) = \\sum_{s \\in S} d(s) \\sum_{a \\in A} \\pi_\\theta (s,a) \\nabla_\\theta log \\pi_\\theta (s,a) R_{s,a}$\n",
    "* $= E_{\\pi_\\theta} [\\nabla_\\theta log \\pi_\\theta (s,a) r]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy gradient theorem\n",
    "* replace instantaneous reward r with long term reward $Q^\\pi (s,a)$\n",
    "* policy gradient is $\\nabla_\\theta J(\\theta) = E_{\\pi_\\theta} [\\nabla_\\theta log \\pi_\\theta (s,a) Q^{\\pi_\\theta} (s,a)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce variance via a critic\n",
    "* monte carlo policy gradient has high variance\n",
    "* use a critic to estimate action-value fn $Q_w (s,a) = Q^{}$\n",
    "* actor-critic maintains 2 sets of parameters \n",
    "    * critic - updates action-value fn parameters w \n",
    "    * actor - updates policy parameter $\\theta$ in the direction suggested by the critic\n",
    "* actor-critic algorithms follow an approximate policy gradient\n",
    "    * $\\nabla_\\theta J(\\theta) = E_{\\pi_\\theta} [\\nabla_\\theta log \\pi_\\theta (s,a) Q_w (s,a)]$\n",
    "    * $\\Delta \\theta = \\alpha \\nabla_\\theta log \\pi_\\theta (s,a) Q_w (s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimating action-val fn\n",
    "* basically policy evaluation\n",
    "* think: monte-carlo policy eval, temporal-diff, $TD(\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
