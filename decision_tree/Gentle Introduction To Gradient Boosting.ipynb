{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[src](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gradient boosting can also do ranking\n",
    "* gradient boosting = gradient descent + boosting\n",
    "* boosting does $\\sum_t \\rho_t h_t(x)$\n",
    "    * in gradient boosting, shortcomings are identified by gradients instead of high weights for samples\n",
    "* easiest to hardest: regression, classification, ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression\n",
    "* regression - fit $(x1, y1 - F(x1)), (x2, y2 - F(x2))$...etc.\n",
    "* gradient descent - $\\theta_i = \\theta_i - \\rho \\frac{dJ}{d\\theta_i}$\n",
    "* regression - $L(y, F(x)) = \\frac{1}{2} (y - F(x))^2$\n",
    "    * $\\frac{dJ}{dF(x_i)} = \\frac{d \\sum_i L(y_i, F(x_i))}{dF(x_i)} = \\frac{dL(y_i, F(x_i))}{dF(x_i)} = F(x_i) - y$\n",
    "        * we can interpret this as the negative gradient\n",
    "    * note that in LR, we are doing $\\frac{dJ}{d\\theta}$, but here we are doing $\\frac{dJ}{dF}$, and that decision trees are not differentiable\n",
    "* similar to gradient descent\n",
    "    * $F(x) = F(x) + h(x_i)$\n",
    "    * $F(x) = F(x) + y - F(x)$\n",
    "    * $F(x) = F(x) - 1 \\frac{dJ}{dF(x)}$\n",
    "* for regression with square loss, residual = negative gradient\n",
    "    * actually updating model with gradient descent\n",
    "* negative gradient: $-g(x) = - \\frac{dL(y_i. F(x_i)}{dF(x_i)} = y_i - F(x_i)$\n",
    "* for each new tree\n",
    "    * calculate negative gradient $-g(x_i)$\n",
    "    * fit a tree to $-g(x_i)$\n",
    "    * $F = F + \\rho h$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### why not square loss\n",
    "* not robust to outliers (try to fit outliers and compromise fit)\n",
    "* absolute loss $L(y, F) = | y - F |$\n",
    "* huber loss $L(y, F) = $\n",
    "    * $\\frac{1}{2} (y - F)^2$ if $|y - F| <= \\delta$\n",
    "    * $\\delta (|y - F| - \\delta / 2)$ if $|y - F| > \\delta$\n",
    "    * so if difference is small, use square, if big, use lasso\n",
    "* absolute loss \n",
    "    * $g(x) = \\frac{dL}{dF} = -1 if y - F > 0, 1 if y - F < 0$\n",
    "    * $-g(x) = sign(y_i - F(x_i))$ (this is 0 or 1)\n",
    "* huber loss \n",
    "    * $-g(x) = y_i - F(x_i) OR \\delta sign(y_i - F(x_i))$\n",
    "* we should follow negative gradients instead of residuals\n",
    "    * negative gradients pay less attention for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiclass classification example\n",
    "* 26 classes \n",
    "* $P_A(x) = \\frac{e^{F_A(x)}}{\\sum e^{F_c(x)}}$\n",
    "* $P_B(x) = \\frac{e^{F_B(x)}}{\\sum e^{F_c(x)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
