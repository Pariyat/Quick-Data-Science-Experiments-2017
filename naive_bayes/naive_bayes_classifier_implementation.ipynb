{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jason.xie/anaconda/lib/python2.7/site-packages/pandas/computation/__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from load_sms_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, vectorizer = load_spam_dataset(\"~/Downloads/smsspamcollection/SMSSpamCollection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    '''\n",
    "        A binary classifier - P(y | x) = P(x | y) * P(y) / P(x)\n",
    "    '''\n",
    "   \n",
    "    def fit(self, X, y, vocab=None):\n",
    "        self.vocab = vocab\n",
    "        self.priors = np.array([[1 - np.mean(y), np.mean(y)]])\n",
    "        \n",
    "        self.likelihood = np.zeros((X.shape[1], 2)) # P(x | y)\n",
    "        for i in range(X.shape[1]): # go over every word\n",
    "            arr = X[:,i].todense()\n",
    "            for label in [0, 1]:\n",
    "                arr_l = arr[y == label]\n",
    "                self.likelihood[i, label] = (np.sum(arr_l >= 1) + 1.0) / (np.sum(arr_l >= -1) + 2) # simple smoothing\n",
    "                # wow this is so efficient\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred = np.array([self.predictOne(np.array(list(a.flat))) for a in X_test.todense()])\n",
    "        return pred\n",
    "        \n",
    "    def predictOne(self, X):\n",
    "        LL = np.concatenate((self.priors, self.likelihood[X == 1]), axis=0)\n",
    "        logp = np.sum(np.log(LL), axis=0) # e.g. [-30, -28]\n",
    "        return np.exp(logp[1]) / np.sum(np.exp(logp)) # e.g. e^-28 / (e^-30 + e^-28)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = NaiveBayesClassifier()\n",
    "clf.fit(X_train, y_train, vocab=vectorizer.vocabulary_inverted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.7906313   0.02442748  0.13862192 ...,  0.17539357  0.99999998\n",
      "  0.0220277 ]\n",
      "(1839,)\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "print y_test_pred\n",
    "print y_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False ..., False  True False]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# print accuracy_score(y_test, y_test_pred)\n",
    "print y_test_pred >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
