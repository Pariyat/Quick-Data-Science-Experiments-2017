# Quick-Data-Science-Experiments-2017


### current:
* solve logistic regression via iterated reweighed least square (http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/)
* ranking example (https://github.com/Microsoft/LightGBM/tree/master/examples/lambdarank)


### to do:
* bass curve (nls w/ 3.12 pg52 IntroTimeSeries Cowpertwait)
* weight elimination (https://papers.nips.cc/paper/323-generalization-by-weight-elimination-with-application-to-forecasting.pdf)
* hinton diagrams & for linear reg (http://tonysyu.github.io/mpltools/auto_examples/special/plot_hinton.html)
* levelplots interpretation
* online covariance formula (http://rebcabin.github.io/blog/2013/01/22/covariance-matrices/)
* stochastic gradient boosting notes 
* HOG (CV) (http://mccormickml.com/2013/05/09/hog-person-detector-tutorial/)
* ARCH / GARCH tutorial (http://www.quantatrisk.com/2014/10/23/garch11-model-in-python/)
* radial basis function network (RBFN) (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.312&rep=rep1&type=pdf)
* Gauss-Newton method for non-linear least sqaures (http://www.seas.ucla.edu/~vandenbe/103/lectures/nlls.pdf)
* sigmoid (W^T X) operates in the linear range if W^{norm} is very small demo
* ICA 
* semisupervised learning survey (http://pages.cs.wisc.edu/~jerryzhu/pub/sslicml07.pdf)
* square vs huberized squared error loss
* Missing At Random (MAR test) (http://stats.stackexchange.com/questions/11991/are-misses-in-my-data-distributed-completely-at-random)
* PRIM bump hunting
* temporal regression (decaying RSS)
* MARS (pyEarth)
* hierarchical mixture of experts (EM & interpretation)
* normalized LL
* LDA notes (http://obphio.us/pdfs/lda_tutorial.pdf) 
* STL notes
* zero shot learning (https://github.com/MLWave/extremely-simple-one-shot-learning)
* poisson regression
* kmeans w/ categorical data (http://edu.cs.uni-magdeburg.de/EC/lehre/sommersemester-2013/wissenschaftliches-schreiben-in-der-informatik/publikationen-fuer-studentische-vortraege/kMeansMixedCatNum.pdf)
* FTRL note (http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf)
* L-BFGS note
* NMF (how it enforces Non-negativity)
* C5.0 tree & rule interpretation
* collaborative filtering for ordinal scores (http://www.ijcai.org/Proceedings/13/Papers/449.pdf)
* coclustering methods (recommendations)
* stacking via CV pedictions (http://www.kdnuggets.com/2017/02/stacking-models-imropved-predictions.html)
* stochastic search - bumping (XOR decision tree example)
* decision tree missing values (surrogate splits, 9.2.4 ELSL)
* isolation trees (http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf)
* random forest variance formula (p*var + (1 - p)/beta *var
* factor analysis
* churn prediction example (https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/)
* adaboost vs SVM (https://ucb-mids.s3.amazonaws.com/prod/Machine+Learning/Readings/Week+4/ShortIntroToBoosting.pdf)
* bayesian model averaging & BIC
* kernel density classification & kernel smoothing with different local kernels
* rbf regression & gmm classification
* 1NN curse of dim test w/ b-v tradeoff (pg 24, 223)
* LOESS & show that boundary fit is linear
* splines in python, b-splines, thin plate spline
* do bfgs on linear and logistic regression
* prob calibration with covariate shift
* linear discriminant analysis (fisher and gaussian derivations)
* quadratic discriminant analysis (https://www.youtube.com/watch?v=JWozRg_X-Vg)
* reduced-rank regression (canonical correlation analysis)
* compressed sensing (http://web.yonsei.ac.kr/nipi/lectureNote/Compressed%20Sensing%20by%20Romberg%20and%20Wakin.pdf)
* steepest descent (https://www.rose-hulman.edu/~bryan/lottamath/steepest.pdf)
* conjugate gradient (http://sep.stanford.edu/data/media/public/oldreports/sep44/44_14.pdf)
* svd to pca
* gaussian processes test
* asymptotic normality of MLE (var 2nd deriv)
* large scale L1 feature selection with Vowpal Wabbit
* gaussian processes for hyperparam optimization
* breaking news prediction on twitter
* multilingual spam filter


### done:
* dataset shift in classification (http://iwann.ugr.es/2011/pdf/InvitedTalk-FHerrera-IWANN11.pdf)
* probability calibration
* * cohen kappa (https://onlinecourses.science.psu.edu/stat509/node/162)
* qr factorization
* principal component regression
* partial least square
* eigen decomposition tut
* bias-variance example w207_lec_1
* logistic regression training on ratio and weights
* logistic regression covariance of coefficients
* perceptron implementation
* naive bayes spam filter
* decision tree imple
* permutation importance (decision tree)
* ranking item recommendations for a user from matrix factorization

===

### potential tuts:
* different types of FMs
* Spark AllReduce
* lessons from Quora ML
