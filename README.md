# Quick-Data-Science-Experiments-2017


### current:
* solve logistic regression via iterated reweighed least square (http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/)
* robust regression
* ranking example (https://github.com/Microsoft/LightGBM/tree/master/examples/lambdarank)


### to do:
* churn prediction example (https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/)
* adaboost vs SVM (https://ucb-mids.s3.amazonaws.com/prod/Machine+Learning/Readings/Week+4/ShortIntroToBoosting.pdf)
* bayesian model averaging & BIC
* kernel density classification & kernel smoothing with different local kernels
* rbf regression & gmm classification
* 1NN curse of dim test w/ b-v tradeoff (pg 24, 223)
* LOESS & show that boundary fit is linear
* splines in python, b-splines, thin plate spline
* do bfgs on linear and logistic regression
* prob calibration with covariate shift
* linear discriminant analysis (fisher and gaussian derivations)
* quadratic discriminant analysis (https://www.youtube.com/watch?v=JWozRg_X-Vg)
* reduced-rank regression (canonical correlation analysis)
* compressed sensing (http://web.yonsei.ac.kr/nipi/lectureNote/Compressed%20Sensing%20by%20Romberg%20and%20Wakin.pdf)
* steepest descent (https://www.rose-hulman.edu/~bryan/lottamath/steepest.pdf)
* conjugate gradient (http://sep.stanford.edu/data/media/public/oldreports/sep44/44_14.pdf)
* svd to pca
* gaussian processes test
* asymptotic normality of MLE (var 2nd deriv)
* large scale L1 feature selection with Vowpal Wabbit
* gaussian processes for hyperparam optimization
* breaking news prediction on twitter
* multilingual spam filter


### done:
* probability calibration
* qr factorization
* principal component regression
* partial least square
* eigen decomposition tut
* bias-variance example w207_lec_1
* logistic regression training on ratio and weights
* logistic regression covariance of coefficients
* perceptron implementation
* naive bayes spam filter


