{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[src](http://www.cs.mcgill.ca/~dprecup/courses/ML/Lectures/ml-lecture03.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE\n",
    "* $L(\\theta | D) = P(D | \\theta)$\n",
    "* MLE for multinomial\n",
    "    * $\\sum_k N_k log \\theta_k + \\lambda (1 - \\sum \\theta_k)$\n",
    "    * [lagrange multiplier example](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-examples)\n",
    "    * derivative - $\\sum_k \\frac{N_k}{\\theta_k} - \\frac{\\lambda}{(1 - \\sum \\theta_k)}$ (slides 22)\n",
    "* KL div of 2 distributions of X $KL(p, q) = \\sum_x P(x) log \\frac{P(x)}{Q(x)}$\n",
    "* if I have a message, the optimal encoding for a letter is $-log p$\n",
    "    * the expected encoding length is $-\\sum p log p$\n",
    "    * if I believe that the letters are coming from q, then I would do $-sum p log q$\n",
    "    * the wasted bits is $-\\sum p log q - (- \\sum p log p) = \\sum p log \\frac{p}{q} = KL(p, q)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction as inference\n",
    "* $P(x_{n+1} | x_1..x_n) = \\int P(x_{n+1} | \\theta, x_1..x_n) P(\\theta | x_1..x_n)  d \\theta$\n",
    "    * = $\\int P(x_{n+1} | \\theta) P(\\theta | x_1 .. x_n) d \\theta$\n",
    "    * $\\sim \\int P(x_{n+1} | \\theta) P(x_1 .. x_n | \\theta) P(\\theta) d \\theta$\n",
    "* If we have 1 toss, $x_1 = H$, what's $P(x_2 | x_1)$\n",
    "    * $P(x_2 = H | x_1 = H) \\sim \\int P(x_1 = H | \\theta) P(\\theta) P(x_2 = H | \\theta) d \\theta$\n",
    "    * note: $P(\\theta) = 1$, because $\\theta \\sim [0, 1]$ uniform\n",
    "    * $\\int_0^1 \\theta * 1 * \\theta = \\frac{1}{3}$\n",
    "* Similarly, $P(x_2 = T | x_1 = H) \\sim \\int_0^1 \\theta * 1 * (1 - \\theta) = (\\theta - \\theta^2)|^1_0 = \\frac{1}{6}$\n",
    "    * after normalizing we get $P(x_2 = T | x_1 = H) = \\frac{\\frac{1}{6}}{\\frac{1}{3} + \\frac{1}{6}} = \\frac{1}{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dirichlet priors\n",
    "* $P(\\theta) = \\alpha \\sum \\theta_i^{\\beta_i - 1}$\n",
    "* posterior $P(\\theta| D) = P(\\theta) P(D | \\theta) = \\alpha \\prod \\theta_i^{\\beta_i - 1 + N_i}$\n",
    "* recall $\\int P(x | \\theta) P(\\theta | D) d \\theta$\n",
    "* for more, read slide 34\n",
    "\n",
    "### conjugate family\n",
    "* property that the posterior follows the same parametric form as the prior is called conjugacy\n",
    "* e.g. conjugate of dirichlet prior is multinomial MLE\n",
    "* conjugate family is useful for closed form, or online update\n",
    "* $\\beta_i$ in dirichlet can be thought of as imaginary counts, that can be overwhelmed when there are lots of data\n",
    "* dirichlet's posterior has the same form as the prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
