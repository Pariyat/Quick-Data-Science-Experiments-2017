{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[paper](https://hal.archives-ouvertes.fr/inria-00103955/document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OAA vs OAO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SVMs output a score, calibrated confidence measure, not probability\n",
    "\n",
    "### experiment\n",
    "* baseline is MLP (possible to estimate accurate posterior probabilities)\n",
    "* knn-digit - 1.35%, MLP-digit - 0.80%\n",
    "\n",
    "* novel measure: reject a sample if $max_j(P(\\omega_j | x)) < T$\n",
    "    * the threshold T defines the rate of samples rejected and consequently the error rate among the samples accepted\n",
    "    * can graph error-reject tradeoff\n",
    "    * the better the probabilities estimate is, the better the error-reject tradeoff is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* calibrate w/ Platt's scaling \n",
    "    * sigmoid version $$P(\\omega_j | f(x)) = \\frac{1}{1 + exp(B_j f_j(x))}$$\n",
    "    * parameters are estimated to minimize LogLoss\n",
    "    * note: nothing guarantees $\\sum_j P(\\omega_k | f(x)) = 1$\n",
    "    * so we need to also normalize \n",
    "    * softmax version $$ \\frac{exp(P(\\omega_k | f(x)))}{\\sum_j exp(P(\\omega_j | f(x)))} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OAO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* classification of an unknown pattern is done according to the maximum voting , where each SVM votes for one class\n",
    "\n",
    "* Hastie & Tibshirani proposed using an iterative algorithm to estimate the posterior probabilities $p_j = P(\\omega_j | x)$ which minimizes the Kullback-Leibler distance between $r_{jj'} = P(\\omega_j | f_{j,j'})$ and $u_{jj'} = \\frac{p_j}{p_j + p_{j'}}$\n",
    "* so minimize $$ \\sum (r_{jj'} log \\frac{r_{jj'}}{u_{jj'}} + (1 - r_{jj'}) log \\frac{1 - r_{jj'}}{1 - u_{jj'}} ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take aways\n",
    "* OAO is hard... (.........)\n",
    "* Understand Platt's scaling assumptions and why it works w/ SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
